이번 장에서는 가중치 매개변수의 최적값을 탐색하는 최적화와 가중치 매개변수 초깃값과 하이퍼파라미터를 설정하는 방법에 대해 설명 하겠습니다.

신경망 학습의 목적은 손실 함수의 값을 최대한 낮추는 매개변수의 최적값을 찾는 것인데 신경망의 최적화는 매우 어려운 문제고, 또 심층 신경망으로 갈수록 매개변수의 수가 엄청나게 많아져서 정답을 찾기는 거의 불가능합니다.

다음은 교재에서 인상 깊었던 예시인데요. 본론으로 들어가기 전에 먼저 말씀드리겠습니다.

모험가가 있습니다. 광활하고 메마른 산맥을 여행하면서 가장 깊고 깊은 골짜기를 찾아가려 합니다. 그런데 심지어 엄격한 제약이 있습니다. 지도를 보지 않고 눈도 가리고 깊은 곳을 찾아가야 합니다. 눈이 보이지 않으니 믿을 거라곤 발바닥으로 느껴지는 기울기뿐입니다. 지금 서 있는 곳보다 조금 더 기울어진 곳을 찾아 천천히 걸음을 옮기는 수밖에 없습니다.

우리도 최적값을 찾기 위해 이 모험가처럼 손실 함수가 가장 낮은 방향으로 매개변수를 조정해 나가야 합니다. 매우 어려운 문제입니다.

그래도  가장 깊거나 가장 깊은 것으로 보이는 골짜기를 찾아가는 여러 방법이 있고, 대표적으로는 확률적 경사 하강법 즉, SGD가 있습니다.

SGD는 기울어진 방향으로 일정 거리만 가는 단순한 방법으로 학습률과 손실함수의 기울기를 곱한 값을 가중치 매개변수에서 빼면서 값을 갱신합니다.

코드를 보시면 gradient 메서드의 파라미터와 기울기 값을 optimizer의 update 메서드에 전달하여 파라미터를 갱신합니다.

하지만 SGD는 단순한 만큼 단점도 있는데 비등방성 함수에서는 극명하게 나타납니다. 아래 함수를 그래프와 등고선으로 그려보면 가로로 길게 늘어진 형태이고 기울기를 그려보면 y축 방향은 크고 x축 방향은 매우 작습니다. 최솟값이 되는 장소는 x와 y가 0이 되는 지점이지만 기울기 대부분은 이 장소를 가리키지 않습니다.

이 함수에 SGD를 적용하면 이 그림과 같이 심하게 왔다 갔다 하는 모습을 보입니다. 상당히 비효율적이죠?

다음은 이 단점을 개선해 주는 모멘텀에 대해 설명 하겠습니니다.